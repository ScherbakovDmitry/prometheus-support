groups:
- name: alerts.yml
  rules:
  - alert: ClusterDown
    expr: up{job="federation-targets"} == 0
    for: 10m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      description: '{{ $labels.instance }} of job {{ $labels.job }} has been down
        for more than 2 minutes.'
      summary: Instance {{ $labels.instance }} down
  - alert: CoreServices_SidestreamIsNotRunning
    expr: sum_over_time(up{service="sidestream"}[10m]) == 0 and on(machine) sum_over_time(probe_success{service="ssh806"}[20m])
      / 20 >= 0.9 unless on(machine) lame_duck_node == 1
    for: 10m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      description: ""
      summary: ""
  - alert: ScraperMostRecentArchivedFileTimeIsTooOld
    expr: (time() - (scraper_maxrawfiletimearchived{container="scraper-sync"} != 0))
      > (56 * 60 * 60) and on(machine) (time() - process_start_time_seconds{service="sidestream"})
      > (30 * 60 * 60) unless on(machine) lame_duck_node == 1
    for: 2h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      description: Max file mtime for {{ $labels.rsync_url }} is older than 56 hours.
      summary: Scraper max file mtime is too old {{ $labels.rsync_url }}
  - alert: ScraperSyncPresentWithoutScraperCollector
    expr: (scraper_lastcollectionattempt{container="scraper-sync"} unless on(machine,
      experiment, rsync_module) up{container="scraper"})
    for: 3h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      description: ""
      summary: ""
  - alert: ScraperCollectorMissingFromScraperSync
    expr: (up{container="scraper"} unless on(machine, experiment, rsync_module) scraper_lastcollectionattempt{container="scraper-sync"})
    for: 3h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      description: ""
      summary: ""
  - alert: SwitchDownAtSite
    expr: up{job="snmp-targets",site!~".*t$"} == 0 and on(site) probe_success{instance=~"s1.*",module="icmp"}
      == 0
    for: 1d
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{
        $labels.site }}
      hints: The issue could be with the switch itself, or with the transit provider.
      summary: The switch at a site has been unreachable for too long.
  - alert: InventoryConfigurationIsMissing
    expr: absent(up{service="ssh806"}) or absent(up{service="rsyncd"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      description: Machine or rsyncd service configuration has been missing for too
        long.
      hints: Check the behavior of the m-lab/operator/.travis.yml deployment, the
        GCS buckets, and the gcp-service-discovery component of prometheus-support.
      summary: Inventory configuration {{ $labels.service }} is missing.
  - alert: InventoryMachinesWithoutRsyncd
    expr: up{service="ssh806"} unless on(machine) up{service="rsyncd"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: Rsyncd configuration is missing from some machines.
  - alert: InventoryRsyncdWithoutMachines
    expr: up{service="rsyncd"} unless on(machine) up{service="ssh806"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: Machine configuration is missing for some rsyncd services.
  - alert: SidestreamServicesAreMissing
    expr: absent(up{service="sidestream"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: ""
  - alert: SidestreamRunningWithoutMachine
    expr: up{service="sidestream"} unless on(machine) up{service="ssh806"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: ""
  - alert: MachineWithoutSidestreamRunning
    expr: up{service="ssh806"} unless on(machine) up{service="sidestream"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: ""
  - alert: ScraperRunningWithoutRsyncd
    expr: up{container="scraper"} unless on(machine, experiment) up{service="rsyncd"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: ""
  - alert: RsyncRunningWithoutScraper
    expr: up{service="rsyncd"} unless on(machine, experiment) up{container="scraper"}
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: ""
      summary: ""
  - alert: DownloaderIsFailingToUpdate
    expr: time() - downloader_last_success_time_seconds > (21 * 60 * 60)
    for: 5m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      hints: Check for errors with the downloader service on grafana with the downloader_Error_Count
        metric, or check the stackdriver logs for the downloader cluster.
      summary: Neither of the last two attempts to download the maxmind/routeviews
        feeds were successful.
  - alert: DownloaderDownOrMissing
    expr: up{container="downloader"} == 0 or absent(up{container="downloader"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: Check the status of Kubernetes clusters on each M-Lab GCP project. Look
        at the travis deployment history for m-lab/downloader.
      summary: The downloader for maxmind/routeviews feeds is down on {{ $labels.instance
        }}.
  - alert: SnmpExporterDownOrMissing
    expr: up{job="snmp-exporter"} == 0 or absent(up{job="snmp-exporter"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The snmp_exporter service runs in a Docker container on a GCE VM named
        'snmp-exporter' in each M-Lab GCP project. Look at the Travis-CI builds/deploys
        for m-lab/snmp-exporter-support, or SSH to the VM and poke around.
      summary: The snmp_exporter service is down on {{ $labels.instance }}.
  - alert: SnmpExporterMissingMetrics
    expr: absent(ifHCOutOctets)
    for: 30m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/
      gcsbucket: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/snmp-targets
      hints: If the snmp_exporter service is running, then there may be a target configuration
        error. Check the target definitions in GCS and the target status in Prometheus.
      prometheus_targets: https://prometheus.mlab-oti.measurementlab.net/targets
      summary: Expected SNMP metrics are missing from Prometheus!
  - alert: SnmpScrapingDownAtSite
    expr: up{job="snmp-targets",site!~".*t$"} == 0 and on(site) probe_success{instance=~"s1.*",module="icmp"}
      == 1
    for: 2h
    labels:
      repo: ops-tracker
      severity: page
    annotations:
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/?orgId=1&var-site_name={{
        $labels.site }}
      hints: Maybe the switch is down? Is the snmp_exporter using the right community
        string? Look in switch-details.json in the m-lab/switch-config repo. Is the
        IP of the snmp_exporter VM in GCE whitelisted on the switch?
      summary: Prometheus is unable to scrape SNMP metrics from a switch.
  - alert: ScriptExporterDownOrMissing
    expr: up{job="script-exporter"} == 0 or absent(up{job="script-exporter"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: 'The script_exporter service runs in a Docker container on a GCE VM named
        ''script-exporter'' in each M-Lab GCP project. For deployment details and
        troubleshooting, you can usually figure out the issue by looking through the
        Travis-CI build logs: https://travis-ci.org/m-lab/script-exporter-support.
        You can also look for hints in the dashboard for the GCE instance, or by SSHing
        to the instance itself.'
      summary: The script_exporter service is down on {{ $labels.instance }}.
  - alert: ScriptExporterMissingMetrics
    expr: absent(script_success{service="ndt_e2e"}) or absent(script_success{service="ndt_queue"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/SuqnZ6Hiz/
      gcsbucket: https://console.cloud.google.com/storage/browser/operator-mlab-oti/prometheus/script-targets
      hints: If the script_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target status
        in Prometheus.
      prometheus_targets: http://status.mlab-oti.measurementlab.net:9090/targets
      summary: Expected script_exporter metrics are missing from Prometheus!
  - alert: BlackboxExporterIpv4DownOrMissing
    expr: up{job="blackbox-exporter-ipv4"} == 0 or absent(up{job="blackbox-exporter-ipv4"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: Check the status of the blackbox-server pod in the prometheus-federation
        cluster on each M-Lab GCP project.
      summary: The blackbox_exporter service is down for IPv4 probes.
  - alert: BlackboxExporterIpv6DownOrMissing
    expr: up{job="blackbox-exporter-ipv6"} == 0 or absent(up{job="blackbox-exporter-ipv6"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The blackbox_exporter for IPv6 checks runs in a Linode VM. Make sure
        the VM is up and running. If it is, check the status of the BBE container
        running in the VM. Domains for VMs are like blackbox-exporter-ipv6.<project>.measurementlab.net.
      summary: The blackbox_exporter service is down or missing for IPv6 probes.
  - alert: TooManyNdtServersDown
    expr: scalar(count(probe_success{service="ndt_raw"} and on(machine) up{service="nodeexporter"}
      == 1 unless on(machine) lame_duck_node == 1 unless on(machine) (probe_success{service="ndt_raw"}
      == 1 and on(machine) probe_success{service="ndt_ssl"} == 1 and on(machine) script_success{service="ndt_e2e"}
      == 1 and on(machine) vdlimit_used{experiment="ndt.iupui"} / vdlimit_total{experiment="ndt.iupui"}
      < 0.95))) / count(probe_success{service="ndt_raw"} and on(machine) up{service="nodeexporter"}
      == 1 unless on(machine) lame_duck_node == 1) > 0.25
    for: 30m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/JAq7W6Nmk/
      hints: Make sure that the blackbox_exporter, script_exporter and node_exporters
        are all working as expected. Was any update to the platform just released?
      summary: Too large a percentage of NDT servers are down.
  - alert: NdtMetricsMissing
    expr: absent(probe_success{service="ndt_raw"}) or absent(probe_success{service="ndt_raw_ipv6"})
      or absent(probe_success{service="ndt_ssl"}) or absent(probe_success{service="ndt_ssl_ipv6"})
      or absent(vdlimit_used{experiment="ndt.iupui"}) or absent(vdlimit_total{experiment="ndt.iupui"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      hints: If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target status
        in Prometheus. vdlimit_* metrics are provided by node_exporter on each node.
      prometheus_targets: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets
      summary: A metric for an NDT service is missing.
  - alert: NeubotMetricsMissing
    expr: absent(probe_success{service="neubot"}) or absent(probe_success{service="neubot_ipv6"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target status
        in Prometheus.
      prometheus_targets: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets
      summary: A metric for a Neubot service is missing.
  - alert: MobiperfMetricsMissing
    expr: absent(probe_success{service="mobiperf"}) or absent(probe_success{service="mobiperf_ipv6"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: If the blackbox_exporter service is running, then there may be a target
        configuration error. Check the target definitions in GCS and the target status
        in Prometheus.
      prometheus_targets: https://prometheus.mlab-oti.measurementlab.net/targets#job-blackbox-targets
      summary: A metric for a Mobiperf service is missing.
  - alert: LameDuckMetricMissingForNode
    expr: up{service="nodeexporter"} == 1 unless on(machine) lame_duck_node
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: Check /var/spool/node_exporter/ on the node to see if the file lame_duck.prom
        is missing. If it is, use the mlabops Ansible lame-duck.yaml playbook to restore
        it.
      summary: Some number of nodes are missing lame-duck status metrics.
  - alert: VdlimitMetricsMissingForNode
    expr: up{service="nodeexporter"} == 1 unless on(machine) (vdlimit_used and vdlimit_total)
    for: 30m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      dashboard: https://grafana.mlab-sandbox.measurementlab.net/d/JAq7W6Nmk/
      hints: Check /var/spool/node_exporter/ on the node to see if the file vdlimit.prom
        is missing. The file is created by /etc/cron.d/prom_vdlimit_metrics.cron.
      summary: Some vdlimit_* metrics are missing.
  - alert: CoreServices_CollectdMlabDown
    expr: collectd_mlab_success == 0
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The collectd-mlab service runs in the mlab_utility slice. Try running
        the ansible/disco/update-mlab-utility.yaml Ansible playbook in the mlabops
        repository to configure collectd-mlab. Login to the node and run the check
        script manually to see what the specific error is (/usr/lib/nagios/plugins/check_collectd_mlab.py).
      summary: A collectd-mlab service is down.
  - alert: CoreServices_CollectdMlabMissing
    expr: up{service="nodeexporter"} == 1 unless on(machine) collectd_mlab_success
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The collectd-mlab service runs in the mlab_utility slice. Try running
        the ansible/disco/update-mlab-utility.yaml Ansible playbook in the mlabops
        repository to configure collectd-mlab. Login to the node and run the check
        script manually to see what the specific error is (/usr/lib/nagios/plugins/check_collectd_mlab.py).
      summary: A collectd-mlab service metric is missing.
  - alert: ParserDailyVolumeTooLow
    expr: candidate_service:etl_test_count:increase24h < (0.7 * quantile by(service)
      (0.5, label_replace(candidate_service:etl_test_count:increase24h offset 1d,
      "delay", "1d", "", ".*") or label_replace(candidate_service:etl_test_count:increase24h
      offset 3d, "delay", "3d", "", ".*") or label_replace(candidate_service:etl_test_count:increase24h
      offset 5d, "delay", "5d", "", ".*") or label_replace(candidate_service:etl_test_count:increase24h
      offset 1w, "delay", "7d", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c1", "", ".*"), "service", "etl-disco-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c2", "", ".*"), "service", "etl-disco-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c1", "", ".*"), "service", "etl-ndt-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c2", "", ".*"), "service", "etl-ndt-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c1", "", ".*"), "service", "etl-sidestream-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c2", "", ".*"), "service", "etl-sidestream-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c1", "", ".*"), "service", "etl-traceroute-parser", "", ".*") or label_replace(label_replace(vector(0),
      "delay", "c2", "", ".*"), "service", "etl-traceroute-parser", "", ".*")))
    for: 2h
    labels:
      repo: dev-tracker
      severity: page
    annotations:
      dashboard: https://grafana.mlab-oti.measurementlab.net/d/PKqnWeNmz/
      hints: Are machines online? Is data being collected? Is the parser working?
      summary: Today's test volume is less than 70% of nominal daily volume.
  - alert: NodeExporterOnEbDownOrMissing
    expr: up{job="eb-node-exporter"} == 0 or absent(up{job="eb-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      hints: Login to EB to see if it is in fact crashed. If so, look through the
        logs.
      summary: The node_exporter instance running on eb.measurementlab.net is down.
  - alert: NodeExporterOnMirrorDownOrMissing
    expr: up{job="mirror-node-exporter"} == 0 or absent(up{job="mirror-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      hints: Login to to see if it is in fact crashed. If so, look through the logs.
      summary: The node_exporter instance running on mirror.measurementlab.net is
        down.
  - alert: NodeExporterOnDnsDownOrMissing
    expr: up{job="dns-node-exporter"} == 0 or absent(up{job="dns-node-exporter"})
    for: 10m
    labels:
      repo: ops-tracker
      severity: ticket
    annotations:
      hints: Login to to see if it is in fact crashed. If so, look through the logs.
      summary: The node_exporter instance running on dns.measurementlab.net is down.
  - alert: GardenerDownOrMissing
    expr: up{container="etl-gardener",instance=~".*:9090"} == 0 or absent(up{container="etl-gardener",instance=~".*:9090"})
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The Gardener runs in the data-processing-cluster
      summary: The ETL Gardener instance is down on {{ $labels.instance }}
  - alert: ETL_ParserPanicNonZero
    expr: etl_panic_count > 0
    for: 10m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: Bugs cause panics. This bug should be fixed. Parsers run in AppEngine.
        Check logs to see the panic stack trace. Identify the archive that led to
        the panic (logs or TaskQueue tasks with many retries). Fix the bug or create
        a new issue describing the failure and linking to the triggering archive.
      summary: An ETL parser panicked {{ $labels.instance }}
  - alert: ETL_AnnotationDownOrMissing
    expr: up{service="annotator"} == 0 or absent(up{service="annotator"})
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The annotator runs in AppEngine. Check logs and recent deployments. The
        daily and batch parsers may also be affected.
      summary: An ETL Annotation Server is offline or missing!
  - alert: NDT_AnnotationRatioTooLow
    expr: bq_ndt_annotation_success / bq_ndt_annotation_total < 0.99 or absent(bq_ndt_annotation_success
      / bq_ndt_annotation_total)
    for: 30m
    labels:
      repo: dev-tracker
      severity: ticket
    annotations:
      hints: The annotator runs in AppEngine. Check logs and recent deployments. The
        daily and batch parsers may also be affected.
      summary: Too many NDT tests are missing annotations!
